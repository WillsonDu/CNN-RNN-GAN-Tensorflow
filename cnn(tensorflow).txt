import tensorflow as tf
import tensorflow.examples.tutorials.mnist.input_data as input_data

mnist = input_data.read_data_sets("MNIST_data", one_hot=True)


class CnnNet:

    def __init__(self):
        self.x = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1])
        self.y = tf.placeholder(dtype=tf.float32, shape=[None, 10])

        # 卷积层参数
        # 卷积核大小(高、宽、输入通道、输出通道)
        self.conv1_w = tf.Variable(tf.truncated_normal([3, 3, 1, 8], stddev=0.1))
        self.conv1_b = tf.Variable(tf.zeros([8]))

        self.conv2_w = tf.Variable(tf.truncated_normal([3, 3, 8, 16], stddev=0.1))
        self.conv2_b = tf.Variable(tf.zeros([16]))

        # 全连接层参数
        self.w1 = tf.Variable(tf.truncated_normal([7 * 7 * 16, 64]))
        self.b1 = tf.Variable(tf.zeros([64]))

        # 输出层
        self.w2 = tf.Variable(tf.truncated_normal([64, 10]))
        self.b2 = tf.Variable(tf.zeros([10]))

    # 添加卷积层(含池化层)
    def add_conv_layer(self, input, filter, bias, conv_stride_shape=None, pool_shape=None, pool_stride_shape=None):
        if conv_stride_shape == None:
            conv_stride_shape = [1, 1]

        if pool_shape == None:
            pool_shape = [2, 2]

        if pool_stride_shape == None:
            pool_stride_shape = pool_shape.copy()

        # 步长[1,H方向上的步长,W方向上的步长,1]
        x = tf.nn.conv2d(input, filter, strides=[1, conv_stride_shape[0], conv_stride_shape[1], 1],
                         padding="SAME") + bias
        return tf.nn.max_pool(tf.nn.leaky_relu(x), ksize=[1, pool_shape[0], pool_shape[1], 1],
                              strides=[1, pool_stride_shape[0], pool_stride_shape[1], 1], padding="SAME")

    def forward(self):
        x = self.add_conv_layer(self.x, self.conv1_w, self.conv1_b)
        x = self.add_conv_layer(x, self.conv2_w, self.conv2_b)

        shape = x.shape
        multi = multi_rest(shape[1:])
        x = tf.reshape(x, [-1, multi])

        x = tf.nn.leaky_relu(tf.matmul(x, self.w1) + self.b1)
        self.output = tf.matmul(x, self.w2) + self.b2
        return self.output

    def backward(self):
        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.output, labels=self.y))
        self.opt = tf.train.AdamOptimizer().minimize(self.loss)

        self.correct_ = tf.equal(tf.argmax(self.output, 1), tf.argmax(self.y, 1))
        self.accuracy = tf.reduce_mean(tf.cast(self.correct_, tf.float32))


def multi_rest(array):
    try:
        total = 1
        for i in array:
            total *= i

        return int(total)
    except:
        raise Exception("type error")


if __name__ == '__main__':
    net = CnnNet()
    net.forward()
    net.backward()

    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)

        for i in range(10000):

            xs, ys = mnist.train.next_batch(128)
            xs = xs.reshape([-1, 28, 28, 1])
            accuracy, _, output, y_lable = sess.run([net.accuracy, net.opt, net.output, net.y],
                                                    feed_dict={net.x: xs, net.y: ys})

            if i % 100 == 0:
                print(accuracy)
                # print("1111111111111111111111111111111111111111")
                # for ii in range(128):
                #     print(output[ii])
                #     print("22222222222222222222222222222222222")
                #     print(y_lable[ii])
                #     print("33333333333333333333333333333333333")
